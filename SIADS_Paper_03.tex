% SIAM Article Template
\documentclass[review]{siamonline1116}

% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.

\input{ex_shared}

% Optional PDF information
\ifpdf
\hypersetup{
  pdftitle={\TheTitle},
  pdfauthor={\TheAuthors}
}
\fi

% The next statement enables references to information in the
% supplement. See the xr-hyperref package for details.

\externaldocument{ex_supplement}

% FundRef data to be entered by SIAM
%<funding-group>
%<award-group>
%<funding-source>
%<named-content content-type="funder-name"> 
%</named-content> 
%<named-content content-type="funder-identifier"> 
%</named-content>
%</funding-source>
%<award-id> </award-id>
%</award-group>
%</funding-group>



\title{Parameter Estimation for JSPAM Galactic Merger Simulations Using a Modified Metropolis-Hastings Method}
\author{Graham West, John Wallin, Zach Sinkala}


\begin{document}

\maketitle


% REQUIRED
\begin{abstract}
Gravitational N-body models can used to model the dynamical properties of interacting galaxies.  These models represent states in a six-dimensional phase space with three-dimensional projections that can observed with astronomical telescopes.  Although these models have thousands of particles, the dynamical state that describes the system can be represented with twelve dynamical parameters.
In this paper, we present the implementation and results of an MCMC-type optimization method to solve the problem of estimating these dynamical parameters for galaxy mergers. Using an adaptive Metropolis-Hastings algorithm combined with the appropriate error function, we are able to find a merger simulation of minimum error with respect to a given merger target. We also explore the connections between the convergence of the method and the sensitivity of the parameters in this system.
\end{abstract}

% REQUIRED
\begin{keywords}
non-linear dynamics, cosmology, MCMC, galaxy mergers, simulation, optimization
\end{keywords}

% REQUIRED
\begin{AMS}
  68Q25, 68R10, 68U05
\end{AMS}


\section{Introduction}
Understanding the morphology and evolution of galactic mergers is a central issue in contemporary astrophysicis--in particular, the inverse problem of tracing a merger back in time to determine its dynamical history. Due to the highly non-linear nature of galactic systems as well as limited observational knowledge of a given system's current state, this problem cannot be solved by simply reversing time in an N-body simulation, necessitating the use search algorithms which require a great number of merger simulations to fully explore the parameter space.

In order to resolve this problem, we need three things: 1) a fast simulation code, 2) an error function which accurately measures the differences between two mergers, and 3) an optimization algorithm which can minimize that error function. Step 1 was completed by Wallin et al. \cite{jspam} with the development of the simulation code entitled JSPAM. Steps 2 and 3 both will be described in this paper since an optimization problem is intimiately linked to its error function.

%(Throughout this paper, we will speak of \textit{minimizing} the fitness function since it is in the form of an error function, i.e., a low value means a high fitness.) 

In what follows, we will discuss the tools and data we had at our disposal, including JSPAM, \textit{Galaxy Zoo: Mergers} and \textit{Merger Wars}, and the SDSS and MaNGA surveys. Then--in the majority of the paper--we will examine our method of calculating the error function and the MCMC algorithm which minimizes it.   We will show how this algorithm converges on some sample systems, and explore how the sensitivity of the dynamical parameters that define our system are related to the convergence rate of the MCMC method.  Finally, we will discuss several future steps which we will take to integrate the MaNGA data into our process so that we can begin fitting real mergers.


\subsection{JSPAM}

Due to the nature of gravitation as a pair-wise interaction and the structure of galaxies being primarily a cloud of massive, interacting particles, the most direct way to simulate a merger would be to set up a full $O(n^2)$ N-body code. Approximations can be made which reduce the complexity to $O(n\hspace{1pt}$log$(n))$--hierarchical tree codes, for example--but, even this is too slow for our purposes. Due to the great quantity of simulations needed to be performed, we required a code which was $O(n)$. Thus, we used the restricted three-body JSPAM code developed by Wallin et al. \cite{jspam}.

This code makes several significant simplifications for the sake of runtime, but since the mergers are not simulated over a long time interval, accuracy is retained.  A detailed description of the code is contained in \cite{jspam}, so we will provide only a brief overview of its functionality. Within the code, galaxies themselves are comprised of two parts: a center of mass and a swarm of massless particles. The massless particles only interact with the masses of the two galaxies while the centers of mass interact with both each other and the particles.  Initially these particles are placed in a single disk of circular orbits in specified orbital plane around the center of mass of their respective galaxy.  The gravitational fields
%(say field or potential?)
around the center of masses of the galaxies are spherically symmetric.  These fields are modeled after the disk, bulge and halo potential described by Hernquist (1988?) used to model the self-consistent gravitational fields of disk galaxies.

Say the mass ratio is prim/sec

The reference frame of the simulation is setup so that one galaxy (called the primary) is initially at the origin with zero velocity and the other galaxy (called the secondary) is given a relative position and velocity.  The equation that describes the acceleration of the ith particle is at position $r_i$ is:
% big equation needed here
\begin{equation}
\vec{a}_i = - \frac{G M_1(| \vec{r}_i - \vec{R}_1|)}{ (\vec{r}_i - \vec{R}_1)^2}\hat{r}_{i1} -
\frac{G M_2(| \vec{r}_i - \vec{R}_2|)}{ (\vec{r}_i - \vec{R}_2)^2}\hat{r}_{i2} 
\end{equation}
where $M_1(r)$ and $M_2(r)$ are the cumulative mass distributions of the two galaxies, $R_1$ and $R_2$ and the center of mass positions, and G is the universal gravitational constant.

The list of dynamical parameters which describe the merger include these relative positions and velocities, the masses of the galaxies, their radii, and their orientations in terms of altitude and azimuth (w.r.t. the $z$-axis), for a total of 14 parameters.  Since the projected position of the two galaxies are constrained by astronomical observations (x, y are always known), there are 12 free parameters in the simulations.

\begin{figure}[!h]
	\centering
	\begin{align*}
		\includegraphics[scale = 0.35]{JSPAMTestPlot_xyz} & \includegraphics[scale = 0.35]{JSPAMTestPlot_xyvz}
	\end{align*}
	\caption{Final time-step of a JSPAM merger simulation (1000 particles per galaxy). Left: X-Y-Z plot. Right X-Y-Vz plot. NOTE: the graphs are viewed at two different orientations.}
\end{figure}

Given a list of \emph{final} parameters post-merger, the centers of mass are placed at their respective positions and integrated backward in time, past the point of closest approach to a fixed starting time. Then, particle positions and velocities as well as the galactic potential are then initialized. Finally, the system is integrated forward in time, with the centers of mass back to their original positions. Since the galactic potential is a fairly complicated function which must be called many times throughout a simulation, it is assumed to be spherically symmetric, pre-calculated at a set of fixed radial distances, and stored. During simulations, the value of the potential in between these radii is calculated via linear interpolation. Since the potentials are stored, they do not change over time; however, since they originate from the centers of mass, they do translate through space. In addition to the potential, JSPAM also includes a force due to dynamical friction. This phenomenon slows objects traveling through a medium of gravitating particles by causing an increase in particle density behind the object, thus increasing the gravitational force in the retrograde direction.

\subsection{Galaxy Zoo: Mergers and Merger Wars}

From a computational perspective, the problem of finding an appropriate function for comparing morphologies is a real challenge. It is difficult to find a method that is robust enough to perform on a level equivalent to that of a human. By nature, humans are excellent at pattern recognition; and where it can be difficult to ``train'' an algorithm to spot similarities between objects, humans can do it almost instinctively. For this reason, Holincheck et al. \cite{citizen} employed the help of thousands of citizen scientists to assist the \emph{Galaxy Zoo: Mergers} project in which they volunteered their pattern recognition abilities to determine best-fit models for 62 actual mergers.

The volunteers analyzed the morphologies of many thousands of simulations via a vote-based tournament scheme called the \emph{Merger Wars} algorithm. In a typical session, a volunteer is shown an image of the target (the actual merger) along with two simulations which model the target. The volunteer then chooses which simulation is most similar to the target, which counts as a ``win'' for one and a ``loss'' for the other. (They can also select neither image, thereby affording both images a loss.) Since it is entirely possible for images to be judged poorly, every simulation participates in multiple rounds of the tournament so that a bad round does not have a large impact on the results of the competition. The fitness of a particular simulation is then calculated as the percentage of ``wins'' it achieved, with the maximum being 1. Since an integral part of our research involved find an explicit mathematical fitness function for the merger morphologies, it was very useful to have the \emph{Merger Wars} data with which we could assess the validity of our own data.

\subsection{SDSS and MaNGA}

The end goal of our research is to construct an automated pipeline which can compare the morphologies of simulated mergers with images of actual mergers. Once we reach this stage, we will use the Sloan Digital Sky Survey's (SDSS) extensive catalog of observational data. Over the years, SDSS has provided many large, detailed data releases on collections of galactic mergers, the latest of which includes data from the Mapping Nearby Galaxies at APO (MaNGA) project. Thanks to the use of new observational tools, MaNGA has been able to capture spectra over the surface of nearly 2,000 galaxies, including many mergers. With this information, it is possible to derive the $z$-velocity fields and dispersions of the galaxies ($z$ being the axis from the telescope to the galaxy with positive $z$ in the direction of the telescope) and using these velocity fields, we will be able to apply our MCMC method for estimating the mergers' dynamical parameters.

\begin{figure}[!h]
	\centering
	\begin{align*}
		\includegraphics[scale = 0.38]{heartgalaxy} & \hspace{5mm}
		\includegraphics[scale = 0.265]{heartgalaxy_velocity}
	\end{align*}
	\caption{Visible image ($512 \times 512$ pixels) and $z$-velocity ($54 \times 54$ pixels) of a merger observed by MaNGA.}
\end{figure}

\section{Methods}

At its heart, a parameter estimation problem is essentially an optimization problem in which one minimizes a given error funciton w.r.t. a set of parameters. The three fundamental parts to the solution of such an optimization problem are 1) a method to visualize the simulations   2) a meaningful error function and 3) a robust optimization algorithm. In this section, we will examine the reasoning behind the function we developed as well as the modifications we made to the canonical Metropolis-Hastings method.

\subsection{Visualizing the Galaxy}

There are essentially two types of information which our error function must capture: particle positions and velocities. Obviously, however, we can't evalute our function on a per-particle basis because--among many reasons--once we begin using real merger targets, we will not have per-star information. It was very important that we kept the context of real merger targets in our minds as we experimented with different functions because whatever we decided on in the testing phase must still be applicable to real targets. Fortunately, this restriction led us to an elegant solution to the problem. The MaNGA data is in the format of a $54 \times 54$ image for the $z$-velocity and a $512 \times 512$ image for visible light intensity. Therefore, simply binning the particle number and $z$-velocity of a simulation will give us an equivalent format.

In the original {\it Galaxy Zoo: Mergers} project, the center of mass positions of the two galaxies were measured directly from astronomical images.  This measurement insures that the centroids of our simulations will align with the target galaxies from these images.
% mention alignment - done

Binning the particle number is trivial; however, when binning the $z$-velocity, we must account for the opacity of gas and dust within the galaxies. To do so, we bin the velocities in 3D, averageing the values of all the particles within each bin to get its value. The equation which governs the decrease in intensity of light as it passes through a medium of density $\rho$ from $z_-$ to $z_+$ is:
\begin{equation}
	I(x,y,z_-,z_+) = I_0(x,y,z_-) \textrm{\hspace{2pt}exp}(-\kappa \int_{z_-}^{z_+} \rho(x,y,z') dz')
\end{equation}
where $\kappa$ is the coefficent of opacity, $I_0$ is the the actual intensity of light generated at a point, and $I$ is the intensity observed after the light has passed through the medium. Now, we use this as a kernel weighting for the velocity to map it from 2D to 3D:
\begin{equation}
	v^{2D}(x,y) = \dfrac{\int_{z_-}^{z_+}v^{3D}(x,y,z)I(x,y,z,z_+)dz}{\int_{z_-}^{z_+}I(x,y,z,z_+)dz}
	\end{equation}
We will now move from a continuous representation of the system using $x, y, z$ to a discrete representation using the indices   $i, j, k$.   Thus, the variable $v_{i,j,k}$ is the mean velocity of the particles  and $n_{i,j,k}$ is the particle count in the bin located at $x_i, y_j, z_k$.  We will also treat the function $I_0$ as proportional to the the particle count $n$ in a single bin:
\begin{gather}
	v^{2D}_{i,j} = \dfrac{\sum_{k = 0}^{n_{bin}-1}v^{3D}_{i,j,k}n_{i,j,k}w_{i,j,k}}{\sum_{k = 0}^{n_{bin}-1}n_{i,j,k}w_{i,j,k}} \\
	w_{i,j,k} = \textrm{exp}(-\kappa \sum_{l = k}^{n_{bin}-1} \dfrac{n_{i,j,l}/n_{pts}}{dxdydz}dz )
\end{gather}

where $n_{bin}$ is the number of bins on one axis, $n_{pts}$ is the total number of particles (included for normalization purposes), and $dx, dy, dz$ are the dimensions of a single bin.  
% need to say what v_ijk and n_ijk are? - DONE JW
% discretization DONE - jw

\begin{figure}[!h]
	\centering
	\begin{align*}
		\includegraphics[scale = 0.3]{Plot_GalaxyPoints.jpg} & \includegraphics[scale = 0.3]{Plot_GalaxyPoints_Binned.jpg}
	\end{align*}
	\includegraphics[scale = 0.3]{Plot_GalaxyVelocity_Binned.jpg}
		\caption{Left: a per-particle image of a merger simulation. Right: a 50x50 square-binning of the left image (high-density regions are red and low-density regions are blue). Bottom: a 50x50 square-binning of the $z$-velocities.}
\end{figure}

% plot to demonstrate opacity vs no opacity

% differentiate between the intensity of light generated at a point versus that obsevered after opacity

\subsection{The Error Function}

Having binned the simulation, we have two grids representing the $x$-$y$ distribution of particle density and $z$-velocity. We now divide the error function into two parts, one representing the similarity of the mergers' particle distributions and the other the RMS error of the $z$-velocities between them:
\begin{equation}
	f(X, X_0) = (1-OvrFrac) \cdot RMSE
\end{equation}
where,
\begin{equation}
	OvrFrac = \dfrac{|A(X) \cap A(X_0)|}{|A(X)|+|A(X_0)|-|A(X) \cap A(X_0)|}
\end{equation}
and
\begin{equation}
	RMSE^2 = \dfrac{1}{|A(X) \cap A(X_0)|}\sum_{i,j = 0}^{n_{bin}-1} w_{i,j}|v_{i,j}^{2D}(X)-v_{i,j}^{2D}(X_0)|^2
\end{equation}

% JW Note: 
% There is a slight problem here - specifically in the case where there is zero overlap.  In that case, 
% OveFRAC = 1 and RMS^2 = 0
% this means our f(X, X_0) will be 0 because all the weights are zero.
% I think this isn't a real problem for us because the Overlap fraction is always greater than 1 since we align the target galaxy to the 
% same physical coordinates.   However, we should address this in the paper.

In these equations, $X$ and $X_0$ are arguments referring to the \textit{simulation} and \textit{target}, respectively, and $A(\cdot)$ is a function which returns a set of bins from an image of the simulation or target. Consquently, $A(X) \cap A(X_0)$ referrs to the set of bins in which both the simulation and target have particles present and $|A|$ simply refers to the cardinality of the set. The overlap fraction, $OvrFrac$, is the fraction of bins in which both the simulation and the target have particles present. $RMSE$ is the weighted root-mean-squared error between the velocity fields only calculated over the overlapping bins and normalized by the number of overlapping bins (thus, $w_k = 1$ if the pixels overlap, $w_k = 0$ otherwise).

After experimenting with different weighting schemes, we derived one that is able to emphasize regions of the mergers which have been heavily affected by tidal forces producing noticable morphological features:
\begin{equation}
	w_{i,j} = \dfrac{1}{N}\left(\dfrac{n_{i,j}}{n_{max}} + \dfrac{\hat{n}_{i,j}}{\hat{n}_{max}}\right)\left(\dfrac{n_{i,j} - \hat{n}_{i,j}}{n_{i,j} + \hat{n}_{i,j}}\right)^2
\end{equation}
where $N$ is a normalization constant, $n_{i,j}$ is the number of particles in a normal simulation bin, and $\hat{n}_{i,j}$ is the number of particles in a bin from a simulation in which we don't take into account gravitational effects between the two galaxies (essentially placing two perfect spiral galaxies at the final position of the simulation). This lets us determine what work was actually done by the tidal forces to alter the morphology of the merger.

% what order to put these next two paragraphs in???
Although this weighting system is not perfect, it works adequately for systems where was have a measured velocity across the interacting galaxy system.  The MaNGA project in the Sloan Digital Sky Survey {\bf REFERENCE NEEDED} was designed specifcally to measure these quantities for real galaxies.  This project will sample the velocity fields for about 10,000 galaxies over its mission using integrated field spectroscopy.   Although most of the galaxies in the sample will not be gravitationally interacting with a neighbor, we expect to have a sample of a few hundred interacting systems within the next several years.   In addition to providing the velocity field, the mission also provides the ionization state for several different atomic species to allow measurements of quantites such as star formation rate, internal absorption, and temperature of the interstellar medium. 

% converse is the wrong word
It is important to have both the overlap and RMSE terms in the function. Consider the fact that morphologically similar mergers will have have high overlap and low velocity error, but the converse of this statement is not necessarily true. If the overlap is high, there is a proability that the velocity error could still be large, or vice versa. This probability is decreased if we include both velocity and overlap in the calculation.

\subsection{Adaptive Metropolis-Hastings}

% say something about the jump_sigma of the different params before talking about adaptive

Due to the chaotic nature of the merger system, the error lanscape of the parameters space is incredibly complicated, having countless local minima and maxima. Consequently, any naive gradient-based optimization scheme would be completely unreliable. Therefore, we chose a more robust MCMC method called the Metropolis-Hastings algorithm. Given an initial set of parameter values, this method applies random--in our case normally-distributed--perturbations to the parameters. Each parameter $i$ has its own specified $\sigma_i$ for its associated distribution. At each subsequent step, the perturbations are accepted with probability $\alpha$, given by:
\begin{equation}
	\alpha(X', X) = \textrm{min}(1, \dfrac{P(X',X_0)}{P(X,X_0)})
\end{equation}
where $X'$ is the current, perturbed simulation and $X$ is last previously accepted simulation and $P$ is given by its logairthm:
\begin{equation}
	\textrm{log}(P(X,X_0)) = \dfrac{1}{2}\textrm{log}(2 \pi \Delta^2) + \dfrac{f(X,X_0)^2}{2 \Delta^2}
\end{equation}
where $\Delta$ is a value representing the scale of the error. Also, the larger $\Delta$ becomes, the more relaxed the method will be to accept poorer simulations. In the canonical algorithm, the $\sigma_i$ values are fixed; however, we included additional functionality to allow them to adapt if multiple simulations are consequtively rejected. Let $\sigma_{i,j}$ be the $\sigma$ value for the $i$-th parameter after $j$ consequtive rejections. Then,
\begin{equation}
	\sigma_{i,j} = 
	\begin{cases} 
		\sigma_i & j \hspace{2pt} \textrm{mod} \hspace{2pt} 3 = 0 \\
		\sigma_i \cdot  M_j & j \hspace{2pt} \textrm{mod} \hspace{2pt} 3 = 1 \\
		\sigma_i \cdot  m_j & j \hspace{2pt} \textrm{mod} \hspace{2pt} 3 = 2
	\end{cases}
\end{equation}
where,
\begin{gather}
	M_j = (1+a)^{(j+2)/3} \\
	m_j = (1-b)^{(j+1)/3}
\end{gather}
and $0<a,b<1$. These two factors serve to alternate between increasing, decreasing, and resetting the $\sigma$ values so that the MCMC can escape local minima or fall into a thin global minimum quicker. We will discuss the values used for $a$ and $b$ in the next section, along with the impact this modification has on convergence.

% maybe a diagram to explain

%Everything described up to this point can be found in the canonical Metropolis-Hastings algorithm; however, due to the chaotic nature of our error function, we decided to add some extra functionality to assist the convergence of the algorithm. In the canonical algorithm, the $\sigma$ values for the parameter perturbations are fixed, but 

\subsection{Probablity of Convergence Within $n$-Steps}
One of the most important problems we faced was that of determining how well our method converges. We wanted a technique which would account for both how accurately and how quickly convergence occurs as we varied the magnitude of the initial perturbation. Our final product was a technique which could calculate the probability of convergence within a fixed number of steps given an initial perturbation.

Since MCMC methods are stochastic, individual runs tell us nothing about their convergence. We must run many tests and calculate statistics on the collective results. Therefore, our technique for testing consisted of running many instances of the MCMC, all with the same target and initial perturbation and calucating the percentage which converged (i.e., reached a minimum error threshold) so as to calculate an approximation to the convergence probability. This was done on a univariate basis by choosing an interval of evenly-spaced points centered on the target parameter value. The interval's width was chosen to be equal to twice the standard deviation of the top $25\%$ runs' value of that parameter. These points were then repeatedly used as the initial perturbation for the MCMC algorithm. By calcuating this probability and plotting it over the interval, we can visually inspect how much the shape of the error curve impacts the rate of convergence, as well as compare the convergence of different parameters. 


	
\section{Results}
There are several key points concerning different aspects of process which we must discuss before moving on to our results. First--concerning the particle distibution of the galaxies in the simulations--though the true galactic mass distribution is closer to an exponential decay as the radius increases, we chose a uniform density over the whole galaxy so that variations in density would be solely due to tidal disturbances rather than an inherent property of the galaxies, making our weighting scheme much more meaningful.

Second--concerning image resolution in light of bin and particle numbers--clearly the more bins, the more precision with which we can measure the error. However, to ensure that there aren't any false empty bins, this requires more particles, i.e., longer simulation time. To achieve a balance between precision and runtime during testing, we broke up our testing into two main regimes: 1,000 particles per galaxy with $15 \times 15$ bins, and 10,000 particles per galaxy and $54 \times 54$ bins (this is the resolution of the MaNGA data). We would quickly test new modifications to our methods with low precision, but then collect data with high precision.

Lastly--concerning the code itself--while the JSPAM code is written in Fortran, the entirety of the MCMC code is written in Python and parallelized with p-threads in C++. It is organized as follows: There is a top-level Python script which determines which points over the interval to choose as the intial parameters for the MCMC's. This script then calls parallelized C++ code which runs a set number of MCMC trials (written in another Python sciprt) per point. This MCMC script then calls the Fortran JSPAM code which performs a simulation and outputs the particle positions and velocities to a file. These files are used to compute the error at each step. In the end, the MCMC script outputs the Markov chain produced. Finally, by analyzing all of the Markvo chains collectively, we calulate the convergence probabilities.
\begin{figure}[!h]
	\centering
	\includegraphics[scale = 0.5]{Workflow_02}
	\caption{Workflow.}
\end{figure}

\subsection{Example: Heart-shaped galaxy}
Describe the galaxy we are modeling
%JW write this
Interacting ring galaxy - di
- number of models that were run by volunteers
- number of "good models"
- fitness function results 
- issues with the best fit
- real velocity field has been observed by MaNGA

For our purposes, we will use the best fit model as our target system rather than the observations.  This allows us to do additional sampling and to know the exact dynamical parameters. 



\subsection{Error vs. Parameters}
We began with a univariate analysis of the error function, sampling it over the interval dsecribed in section 2.3.  The following plots illustrate the error function's dependence on the 12 parameters. Notice how the minimum is located in the center of the interval, coincident with the target's parameter value.

\begin{table}[!h]
\begin{center}
	\begin{tabular}{ | c || c | c | c | c | }
		\hline
		 & min param value & actual param value & max param value & max error value\\
 		\hline
		\hline
		$z$ & 2.85849 & 3.35734 & 3.85636 & 0.36256\\
		\hline
		$v_x$ & -0.35995 & -0.12979 & 0.10037 & 1.19831\\
		\hline
		$v_y$ & 0.41617 & 0.55971 & 0.70234 & 0.82497\\
		\hline
		$v_z$ & 3.17224 & 4.02072 & 4.86919 & 0.60629\\
		\hline
		mass ratio & 0.46377 & 0.79796 & 1.13216 & 0.46780\\
		\hline
		total mass & 1.28367 & 1.7633 & 2.24293 & 0.30094\\
		\hline
		rad prim & 0.26547 & 0.30541 & 0.34535 & 0.08838\\
		\hline
		rad sec & 0.37291 & 0.41196 & 0.45101 & 0.03025\\
		\hline
		phi prim & 147.39649 & 152.77952 & 158.16255 & 0.01729\\
		\hline
		phi sec & 190.09539 & 195.62801 & 201.16063 & 0.01444\\
		\hline
		theta prim & 140.86124 & 147.22542 & 153.5896 & 0.039687\\
		\hline
		theta sec & 133.44108 & 138.42519 & 143.4093 & 0.02756\\
		\hline
		\hline
	\end{tabular}
	\caption{test}
\end{center}
\end{table}


\begin{figure}[!h]
	\centering
	
	\begin{align*}
		\includegraphics[scale = 0.25]{Error-z} & \includegraphics[scale = 0.25]{Error-vx} & \includegraphics[scale = 0.25]{Error-vy}\\
		\includegraphics[scale = 0.25]{Error-vz} & \includegraphics[scale = 0.25]{Error-massRatio} &\includegraphics[scale = 0.25]{Error-totalMass}\\
		\includegraphics[scale = 0.25]{Error-radPrim} & \includegraphics[scale = 0.25]{Error-radSec} & \includegraphics[scale = 0.25]{Error-phiPrim}\\
		\includegraphics[scale = 0.25]{Error-phiSec} & \includegraphics[scale = 0.25]{Error-thetaPrim} &\includegraphics[scale = 0.25]{Error-thetaSec}\\
	\end{align*}
	\caption{Plots.}
%	plot with alpha, nBin, nPart...
%	During testing, our binning resolution ranged between 15$\times$15 to 54$\times$54 and particle numbers were between 1,000 and 10,000 per galaxy. 
\end{figure}

Notice how the prim/sec's (one of them) variance in error tends to be much larger than the other. This is because thhe prim is the one that is nearer to the observer and thus blocks the view of the other one. Were the opacity coefficient lowered, this effect would be reduced and the galaxies would have more similar error profiles.


\subsection{Physical Convergence - what does it look like?}
Before examining the statistical results from our convergence tests, let's look at pretty pictures.



Density/velocity heat maps, scatter plots
 
 

 
\subsection{Convergence Probabilities}

MCMC plots 


\section{Analysis}







\section{Conclusions}





\section*{Acknowledgments}
We would like to acknowledge the assistance of volunteers in putting
together this example manuscript and supplement.


Funding for the Sloan Digital Sky Survey IV has been provided by the Alfred P. Sloan Foundation, the U.S. Department of Energy Office of Science, and the Participating Institutions. SDSS-IV acknowledges
support and resources from the Center for High-Performance Computing at
the University of Utah. The SDSS web site is www.sdss.org.

SDSS-IV is managed by the Astrophysical Research Consortium for the 
Participating Institutions of the SDSS Collaboration including the 
Brazilian Participation Group, the Carnegie Institution for Science, 
Carnegie Mellon University, the Chilean Participation Group, the French Participation Group, Harvard-Smithsonian Center for Astrophysics, 
Instituto de Astrof\'isica de Canarias, The Johns Hopkins University, 
Kavli Institute for the Physics and Mathematics of the Universe (IPMU) / 
University of Tokyo, Lawrence Berkeley National Laboratory, 
Leibniz Institut f\"ur Astrophysik Potsdam (AIP),  
Max-Planck-Institut f\"ur Astronomie (MPIA Heidelberg), 
Max-Planck-Institut f\"ur Astrophysik (MPA Garching), 
Max-Planck-Institut f\"ur Extraterrestrische Physik (MPE), 
National Astronomical Observatories of China, New Mexico State University, 
New York University, University of Notre Dame, 
Observat\'ario Nacional / MCTI, The Ohio State University, 
Pennsylvania State University, Shanghai Astronomical Observatory, 
United Kingdom Participation Group,
Universidad Nacional Aut\'onoma de M\'exico, University of Arizona, 
University of Colorado Boulder, University of Oxford, University of Portsmouth, 
University of Utah, University of Virginia, University of Washington, University of Wisconsin, 
Vanderbilt University, and Yale University.
%
%
\bibliographystyle{siamplain}
\bibliography{references}

\newcommand{\apj}{apj}
\newcommand{\mnras}{mnras}
\newcommand{\aj}{aj}
\newcommand{ \rmxaa}{ rmxaa}

\begin{thebibliography}{10}

\bibitem{citizen} A. Holincheck, J. Wallin, K. Borne, L. Fortson, C. Lintott, A M. Smith, S. Bamford, W. Keel, and M. Parrish, ``Galaxy Zoo: Mergers - Dynamical Models of Interacting Galaxies.'' \textit{MNRAS}, (2016). Vol. 459, Is. 1, 720-745.

\bibitem{galaxyFE} H. Mo, F. Bosch, and S. White, \textit{Galaxy Formation and Evolution}, 2010, Cambridge University Press.

\bibitem{jspam} J. Wallin, A. Holincheck, and A. Harvey, ``JSPAM: A restricted three-body code for simulating interacting galaxies.'' \textit{Astronomy and Computing} 16, (2016). 26-33.




%Please cite the relevant survey description(s):

%Blanton et al. 2017 (SDSS-IV Overview)
%Bundy et al. 2015 (MaNGA technical description)
%Dawson et al. 2016 (eBOSS technical description)
%Eisenstein et al. 2011 (SDSS-III technical description)
%Dawson et al. 2013 (BOSS technical description)
%York et al. 2000 (SDSS-I and SDSS-II Legacy technical description)
%Yanny et al. 2009 (SEGUE-1 technical description)
%Frieman et al. 2008 (SDSS-II Supernova Survey technical description)

%the Sloan Foundation 2.5-meter Telescope description:
%Gunn et al. 2006

%the relevant Data Release Paper:

%Data Release 14 (Abolfathi et al. 2017; submitted to ApJS; arXiv:1707.09322) is the most recent Data Release paper, but please cite the appropriate paper for the Data Release used for your download.
%and the relevant instrumentation paper(s):

%Gunn et al. 1998 (SDSS photometry, camera)
%Doi et al. 2010 (SDSS photometry, filters)
%Smee et al 2013.   (SDSS original optical spectrograph and the BOSS upgrade to that spectrograph)
%Wilson et al. 2010 (APOGEE spectrograph)
%Drory et al. 2015 (MaNGA Integral Field Unit Fiber Feed System; note that MaNGA uses the BOSS Spectrograph, Smee et al. 2013)





\bibitem[Bizyaev et al.(2017)]{2017ApJ...839...87B} Bizyaev, D., Walterbos, R.~A.~M., Yoachim, P., et al.\ 2017, \apj, 839, 87 


\bibitem[Jin et al.(2016)]{2016MNRAS.463..913J} Jin, Y., Chen, Y., Shi, Y., et al.\ 2016, \mnras, 463, 913 


\bibitem[Law et al.(2016)]{2016AJ....152...83L} Law, D.~R., Cherinka, B., Yan, R., et al.\ 2016, \aj, 152, 83 


\bibitem[S{\'a}nchez et al.(2016)]{2016RMxAA..52..171S} S{\'a}nchez, S.~F., P{\'e}rez, E., S{\'a}nchez-Bl{\'a}zquez, P., et al.\ 2016, \rmxaa, 52, 171 


\bibitem[S{\'a}nchez et al.(2016)]{2016RMxAA..52...21S} S{\'a}nchez, S.~F., P{\'e}rez, E., S{\'a}nchez-Bl{\'a}zquez, P., et al.\ 2016, \rmxaa, 52, 21 


\bibitem[Drory et al.(2015)]{2015AJ....149...77D} Drory, N., MacDonald, N., Bershady, M.~A., et al.\ 2015, \aj, 149, 77 


\bibitem[Bundy et al.(2015)]{2015ApJ...798....7B} Bundy, K., Bershady, M.~A., Law, D.~R., et al.\ 2015, \apj, 798, 7 


\bibitem[York et al.(2000)]{2000AJ....120.1579Y} York, D.~G., Adelman, J., Anderson, J.~E., Jr., et al.\ 2000, \aj, 120, 1579 


\end{thebibliography}


\end{document}




